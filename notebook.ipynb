{"cells":[{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["import os\n","from PIL import Image\n","import numpy as np\n","from math import ceil\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","\n","from keras.applications.vgg16 import VGG16\n","from keras.models import Model\n","from keras.layers import Dense"]},{"cell_type":"markdown","metadata":{},"source":["# Loading the Data"]},{"cell_type":"markdown","metadata":{},"source":["This code loads images from the men and women's clothing datasets from the fashion MNIST. The goal will be distinguishing between these.\n","\n","To map the images to a uniform size, I use Pillow's resize function with cubic interpolation. Pillow will use cubis spline interpolation to build a map of the image, and then it will resample in the desired size. This means that images of any size can be resamples at any other size (although of course you wouldn't want to take this too far). I resample at 250x200 because this seems to strike a good balance of accuracy and speed."]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"data":{"text/plain":["(10, 150528)"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["n_imgs = 10\n","size = 224, 224\n","\n","men_files = [\"data/men/\" + s for s in os.listdir(\"data/men\")[:n_imgs//2]]\n","women_files = [\"data/women/\" + s for s in os.listdir(\"data/women\")[:ceil(n_imgs/2)]]\n","\n","imgs = []\n","for file_name in men_files + women_files:\n","    with Image.open(file_name) as img:\n","        img = img.resize(size, resample=Image.BICUBIC)\n","        imgs.append(np.array(img).reshape(-1))\n","\n","imgs = MinMaxScaler().fit_transform(np.vstack(imgs))\n","\n","imgs.shape"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["y = np.array([0]*len(men_files) + [1]*len(women_files))\n","x_train, x_test, y_train, y_test = train_test_split(imgs, y, test_size=0.2)"]},{"cell_type":"markdown","metadata":{},"source":["# SVM"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["kernel\ttrain_score\ttest_score\n","------\t-----------\t----------\n","linear\t1.0000\t\t0.5000\n","poly\t1.0000\t\t1.0000\n","rbf\t1.0000\t\t1.0000\n"]}],"source":["print(\"kernel\\ttrain_score\\ttest_score\")\n","print(\"------\\t-----------\\t----------\")\n","for kernel in \"linear\", \"poly\", \"rbf\":\n","    model = SVC(kernel=kernel, degree=2)\n","    model.fit(x_train, y_train)\n","    train_score = model.score(x_train, y_train)\n","    test_score = model.score(x_test, y_test)\n","    print(f\"{kernel}\\t{train_score:.4f}\\t\\t{test_score:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Embedding"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["# vgg16 = VGG16(weights='imagenet')"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["x_train = x_train.reshape(-1, *size, 3)\n","x_test = x_test.reshape(-1, *size, 3)"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["embedding_model = Model(inputs=vgg16.input, outputs=vgg16.get_layer('fc2').output)\n","embed_train = embedding_model.predict(x_train)\n","embed_test = embedding_model.predict(x_test)"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["kernel\ttrain_score\ttest_score\n","------\t-----------\t----------\n","linear\t1.0000\t\t0.0000\n","poly\t1.0000\t\t0.5000\n","rbf\t1.0000\t\t0.5000\n"]}],"source":["print(\"kernel\\ttrain_score\\ttest_score\")\n","print(\"------\\t-----------\\t----------\")\n","for kernel in \"linear\", \"poly\", \"rbf\":\n","    model = SVC(kernel=kernel, degree=2)\n","    model.fit(embed_train, y_train)\n","    train_score = model.score(embed_train, y_train)\n","    test_score = model.score(embed_test, y_test)\n","    print(f\"{kernel}\\t{train_score:.4f}\\t\\t{test_score:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Transfer Learning "]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["l1 = Dense(64, activation=\"relu\")(embedding_model.layers[-1].output)\n","l2 = Dense(12, activation=\"relu\")(l1)\n","l3 = Dense(1, activation=\"sigmoid\")(l2)\n","\n","transfer_model = Model(inputs=embedding_model.input, outputs=l3)\n","transfer_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1/1 [==============================] - 2s 2s/step - loss: 0.7111 - accuracy: 0.5000\n","Epoch 2/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.6823 - accuracy: 0.5000\n","Epoch 3/20\n","1/1 [==============================] - 2s 2s/step - loss: 0.8025 - accuracy: 0.5000\n","Epoch 4/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.6098 - accuracy: 0.8750\n","Epoch 5/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.6917 - accuracy: 0.5000\n","Epoch 6/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.5959 - accuracy: 0.5000\n","Epoch 7/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.5588 - accuracy: 0.7500\n","Epoch 8/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.5867 - accuracy: 0.5000\n","Epoch 9/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.5226 - accuracy: 0.7500\n","Epoch 10/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.4833 - accuracy: 1.0000\n","Epoch 11/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.4983 - accuracy: 0.7500\n","Epoch 12/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.4658 - accuracy: 0.8750\n","Epoch 13/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.4196 - accuracy: 1.0000\n","Epoch 14/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.4174 - accuracy: 1.0000\n","Epoch 15/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.4100 - accuracy: 1.0000\n","Epoch 16/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.3718 - accuracy: 1.0000\n","Epoch 17/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.3502 - accuracy: 1.0000\n","Epoch 18/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.3485 - accuracy: 1.0000\n","Epoch 19/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.3299 - accuracy: 1.0000\n","Epoch 20/20\n","1/1 [==============================] - 1s 1s/step - loss: 0.3017 - accuracy: 1.0000\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7fb6e1956460>"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["# transfer learning\n","for layer in embedding_model.layers:\n","    layer.trainable = False\n","for layer in l1, l2, l3:\n","    layer.trainable = True\n","transfer_model.fit(x_train, y_train, epochs=20)"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 1s/step - loss: 0.2903 - accuracy: 1.0000\n","train score: [0.29032576084136963, 1.0]\n","1/1 [==============================] - 0s 436ms/step - loss: 0.6953 - accuracy: 0.5000\n","test score: [0.6953142881393433, 0.5]\n"]}],"source":["print(f\"train score: {transfer_model.evaluate(x_train, y_train)}\")\n","print(f\"test score: {transfer_model.evaluate(x_test, y_test)}\")"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","1/1 [==============================] - 1s 1s/step - loss: 0.2903 - accuracy: 1.0000\n","Epoch 2/5\n","1/1 [==============================] - 1s 1s/step - loss: 0.2840 - accuracy: 1.0000\n","Epoch 3/5\n","1/1 [==============================] - 1s 1s/step - loss: 0.2645 - accuracy: 1.0000\n","Epoch 4/5\n","1/1 [==============================] - 1s 1s/step - loss: 0.2455 - accuracy: 1.0000\n","Epoch 5/5\n","1/1 [==============================] - 1s 1s/step - loss: 0.2377 - accuracy: 1.0000\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7fb6db727100>"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["# fine tuning\n","for layer in embedding_model.layers:\n","    layer.trainable = True\n","transfer_model.fit(x_train, y_train, epochs=5)"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 2s 2s/step - loss: 0.2293 - accuracy: 1.0000\n","train score: [0.22926774621009827, 1.0]\n","1/1 [==============================] - 1s 505ms/step - loss: 0.8172 - accuracy: 0.5000\n","test score: [0.8172279000282288, 0.5]\n"]}],"source":["print(f\"train score: {transfer_model.evaluate(x_train, y_train)}\")\n","print(f\"test score: {transfer_model.evaluate(x_test, y_test)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"author":"Leo Ware","interpreter":{"hash":"66b77bed870dba3c9e402857d29788d70ab8064e505ea7705ee4047aec30bc47"},"kernelspec":{"display_name":"Python 3.9.5 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"orig_nbformat":4,"title":"CS156 - Assignment #4"},"nbformat":4,"nbformat_minor":2}
