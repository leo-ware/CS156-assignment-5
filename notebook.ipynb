{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","from PIL import Image\n","import numpy as np\n","from math import ceil\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","\n","from keras.applications.vgg16 import VGG16\n","from keras.models import Model\n","from keras.layers import Dense"]},{"cell_type":"markdown","metadata":{},"source":["# Loading the Data"]},{"cell_type":"markdown","metadata":{},"source":["This code loads images from the men and women's clothing datasets from the fashion MNIST. The goal will be distinguishing between these.\n","\n","To map the images to a uniform size, I use Pillow's resize function with cubic interpolation. Pillow will use cubis spline interpolation to build a map of the image, and then it will resample in the desired size. This means that images of any size can be resamples at any other size (although of course you wouldn't want to take this too far). I resample at 250x200 because this seems to strike a good balance of accuracy and speed."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["(2512, 150528)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["n_imgs = 3000\n","size = 224, 224\n","\n","men_files = [\"data/men/\" + s for s in os.listdir(\"data/men\")[:n_imgs//2]]\n","women_files = [\"data/women/\" + s for s in os.listdir(\"data/women\")[:ceil(n_imgs/2)]]\n","\n","imgs = []\n","for file_name in men_files + women_files:\n","    with Image.open(file_name) as img:\n","        img = img.resize(size, resample=Image.BICUBIC)\n","        imgs.append(np.array(img).reshape(-1))\n","\n","imgs = MinMaxScaler().fit_transform(np.vstack(imgs))\n","\n","imgs.shape"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["y = np.array([0]*len(men_files) + [1]*len(women_files))\n","x_train, x_test, y_train, y_test = train_test_split(imgs, y, test_size=0.2)"]},{"cell_type":"markdown","metadata":{},"source":["# SVM"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["kernel\ttrain_score\ttest_score\n","------\t-----------\t----------\n","linear\t1.0000\t\t0.5825\n","poly\t0.9457\t\t0.6759\n","rbf\t0.8666\t\t0.6899\n"]}],"source":["print(\"kernel\\ttrain_score\\ttest_score\")\n","print(\"------\\t-----------\\t----------\")\n","for kernel in \"linear\", \"poly\", \"rbf\":\n","    model = SVC(kernel=kernel, degree=2)\n","    model.fit(x_train, y_train)\n","    train_score = model.score(x_train, y_train)\n","    test_score = model.score(x_test, y_test)\n","    print(f\"{kernel}\\t{train_score:.4f}\\t\\t{test_score:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Embedding"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-03-29 05:24:27.227960: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["vgg16 = VGG16(weights='imagenet')"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["x_train = x_train.reshape(-1, *size, 3)\n","x_test = x_test.reshape(-1, *size, 3)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["embedding_model = Model(inputs=vgg16.input, outputs=vgg16.get_layer('fc2').output)\n","embed_train = embedding_model.predict(x_train)\n","embed_test = embedding_model.predict(x_test)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["kernel\ttrain_score\ttest_score\n","------\t-----------\t----------\n","linear\t0.9811\t\t0.7773\n","poly\t0.7894\t\t0.7555\n","rbf\t0.7765\t\t0.7515\n"]}],"source":["print(\"kernel\\ttrain_score\\ttest_score\")\n","print(\"------\\t-----------\\t----------\")\n","for kernel in \"linear\", \"poly\", \"rbf\":\n","    model = SVC(kernel=kernel, degree=2)\n","    model.fit(embed_train, y_train)\n","    train_score = model.score(embed_train, y_train)\n","    test_score = model.score(embed_test, y_test)\n","    print(f\"{kernel}\\t{train_score:.4f}\\t\\t{test_score:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Transfer Learning "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["l1 = Dense(64, activation=\"relu\")(embedding_model.layers[-1].output)\n","l2 = Dense(12, activation=\"relu\")(l1)\n","l3 = Dense(1, activation=\"sigmoid\")(l2)\n","\n","transfer_model = Model(inputs=embedding_model.input, outputs=l3)\n","transfer_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n"," 1/63 [..............................] - ETA: 39:27 - loss: 0.7309 - accuracy: 0.5000"]}],"source":["# transfer learning\n","for layer in embedding_model.layers:\n","    layer.trainable = False\n","for layer in l1, l2, l3:\n","    layer.trainable = True\n","transfer_model.fit(x_train, y_train, epochs=20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 1s/step - loss: 0.2903 - accuracy: 1.0000\n","train score: [0.29032576084136963, 1.0]\n","1/1 [==============================] - 0s 436ms/step - loss: 0.6953 - accuracy: 0.5000\n","test score: [0.6953142881393433, 0.5]\n"]}],"source":["print(f\"train score: {transfer_model.evaluate(x_train, y_train)}\")\n","print(f\"test score: {transfer_model.evaluate(x_test, y_test)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","1/1 [==============================] - 1s 1s/step - loss: 0.2903 - accuracy: 1.0000\n","Epoch 2/5\n","1/1 [==============================] - 1s 1s/step - loss: 0.2840 - accuracy: 1.0000\n","Epoch 3/5\n","1/1 [==============================] - 1s 1s/step - loss: 0.2645 - accuracy: 1.0000\n","Epoch 4/5\n","1/1 [==============================] - 1s 1s/step - loss: 0.2455 - accuracy: 1.0000\n","Epoch 5/5\n","1/1 [==============================] - 1s 1s/step - loss: 0.2377 - accuracy: 1.0000\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7fb6db727100>"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["# fine tuning\n","for layer in embedding_model.layers:\n","    layer.trainable = True\n","transfer_model.fit(x_train, y_train, epochs=5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 2s 2s/step - loss: 0.2293 - accuracy: 1.0000\n","train score: [0.22926774621009827, 1.0]\n","1/1 [==============================] - 1s 505ms/step - loss: 0.8172 - accuracy: 0.5000\n","test score: [0.8172279000282288, 0.5]\n"]}],"source":["print(f\"train score: {transfer_model.evaluate(x_train, y_train)}\")\n","print(f\"test score: {transfer_model.evaluate(x_test, y_test)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"author":"Leo Ware","interpreter":{"hash":"66b77bed870dba3c9e402857d29788d70ab8064e505ea7705ee4047aec30bc47"},"kernelspec":{"display_name":"Python 3.9.5 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"orig_nbformat":4,"title":"CS156 - Assignment #4"},"nbformat":4,"nbformat_minor":2}
